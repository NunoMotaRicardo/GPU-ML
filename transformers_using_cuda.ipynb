{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a008cb3f",
   "metadata": {},
   "source": [
    "# Transformer Tokenizers with CUDA: A Practical Guide\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we explore how to use transformer tokenizers from the HuggingFace `transformers` library with CUDA acceleration. We'll examine different popular transformer models, their memory requirements, and demonstrate practical tokenization examples.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Setting up transformers with CUDA support\n",
    "- Understanding tokenizers and how they work\n",
    "- Comparing different transformer models (BERT, DistilBERT, RoBERTa, and more)\n",
    "- Evaluating memory requirements for various models\n",
    "- Determining which models fit on an RTX 5060 Ti (12GB VRAM)\n",
    "\n",
    "**Why tokenizers matter:**\n",
    "Tokenizers are the first step in any NLP pipeline. They convert raw text into numerical tokens that transformer models can process. Different models use different tokenization strategies (WordPiece, Byte-Pair Encoding, etc.), which affect both performance and vocabulary size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9151daf",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies\n",
    "\n",
    "First, let's import the necessary libraries and check our CUDA availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50976b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cu130\n",
      "CUDA available: True\n",
      "CUDA version: 13.0\n",
      "GPU Device: NVIDIA GeForce RTX 5060 Ti\n",
      "Total GPU Memory: 15.93 GB\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install transformers torch accelerate huggingface_hub python-dotenv\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModel,\n",
    "    BertTokenizer, BertModel,\n",
    "    DistilBertTokenizer, DistilBertModel,\n",
    "    RobertaTokenizer, RobertaModel\n",
    ")\n",
    "from huggingface_hub import login\n",
    "import gc\n",
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Total GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdd9a2c",
   "metadata": {},
   "source": [
    "## 2. HuggingFace Authentication\n",
    "\n",
    "Authenticate with HuggingFace to access models (optional for public models, required for gated models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8b63633",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Successfully authenticated with HuggingFace\n"
     ]
    }
   ],
   "source": [
    "# Load HF token from environment variable\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "if HF_TOKEN:\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"‚úì Successfully authenticated with HuggingFace\")\n",
    "else:\n",
    "    print(\"‚ö† No HF_TOKEN found. Set it in .env file or as environment variable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d969cf0",
   "metadata": {},
   "source": [
    "## 3. Understanding Transformer Models\n",
    "\n",
    "Before diving into tokenization, let's understand the models we'll be working with:\n",
    "\n",
    "### BERT (Bidirectional Encoder Representations from Transformers)\n",
    "- **Tokenizer**: WordPiece tokenization\n",
    "- **Vocabulary Size**: ~30,000 tokens\n",
    "- **Model Sizes**: \n",
    "  - BERT-base: 110M parameters (~440MB)\n",
    "  - BERT-large: 340M parameters (~1.3GB)\n",
    "- **Use Case**: General-purpose NLP tasks, question answering, text classification\n",
    "\n",
    "### DistilBERT (Distilled BERT)\n",
    "- **Tokenizer**: Same WordPiece as BERT\n",
    "- **Vocabulary Size**: ~30,000 tokens\n",
    "- **Model Size**: 66M parameters (~260MB)\n",
    "- **Key Feature**: 40% smaller, 60% faster than BERT-base, retains 97% of performance\n",
    "- **Use Case**: When you need BERT-like performance with lower resource requirements\n",
    "\n",
    "### RoBERTa (Robustly Optimized BERT)\n",
    "- **Tokenizer**: Byte-Pair Encoding (BPE)\n",
    "- **Vocabulary Size**: ~50,000 tokens\n",
    "- **Model Sizes**:\n",
    "  - RoBERTa-base: 125M parameters (~500MB)\n",
    "  - RoBERTa-large: 355M parameters (~1.4GB)\n",
    "- **Key Feature**: Improved training procedure, better performance than BERT\n",
    "- **Use Case**: When you need state-of-the-art performance for language understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c70c5c",
   "metadata": {},
   "source": [
    "### Additional Models to Consider\n",
    "\n",
    "**ALBERT (A Lite BERT)**\n",
    "- Model Size: 12M parameters (~50MB for base)\n",
    "- Even more efficient than DistilBERT through parameter sharing\n",
    "\n",
    "**GPT-2**\n",
    "- Model Sizes: 124M (small) to 1.5B (XL) parameters\n",
    "- Decoder-only architecture, excellent for text generation\n",
    "\n",
    "**T5 (Text-to-Text Transfer Transformer)**\n",
    "- Model Sizes: 60M (small) to 11B (XXL) parameters\n",
    "- Unified text-to-text framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa872f1",
   "metadata": {},
   "source": [
    "## 4. Memory Requirements and RTX 5060 Ti Compatibility\n",
    "\n",
    "Let's analyze which models can run on an RTX 5060 Ti with 12GB VRAM:\n",
    "\n",
    "### Memory Calculation Formula\n",
    "For inference: `Memory ‚âà Model Size √ó 1.2 (overhead) + Batch Size √ó Sequence Length √ó Hidden Size √ó 4 bytes`\n",
    "\n",
    "### Models Compatible with RTX 5060 Ti (12GB VRAM)\n",
    "\n",
    "| Model | Parameters | Model Size | Peak Memory (batch=1, seq=512) | Status |\n",
    "|-------|------------|------------|--------------------------------|--------|\n",
    "| DistilBERT | 66M | ~260MB | ~0.5GB | ‚úÖ Excellent |\n",
    "| BERT-base | 110M | ~440MB | ~0.8GB | ‚úÖ Excellent |\n",
    "| RoBERTa-base | 125M | ~500MB | ~0.9GB | ‚úÖ Excellent |\n",
    "| ALBERT-base | 12M | ~50MB | ~0.3GB | ‚úÖ Excellent |\n",
    "| BERT-large | 340M | ~1.3GB | ~2.5GB | ‚úÖ Good |\n",
    "| RoBERTa-large | 355M | ~1.4GB | ~2.6GB | ‚úÖ Good |\n",
    "| GPT-2 | 124M | ~500MB | ~1.0GB | ‚úÖ Excellent |\n",
    "| GPT-2 Medium | 355M | ~1.4GB | ~2.7GB | ‚úÖ Good |\n",
    "| GPT-2 Large | 774M | ~3.0GB | ~5.5GB | ‚úÖ Acceptable |\n",
    "| T5-base | 220M | ~850MB | ~1.8GB | ‚úÖ Excellent |\n",
    "| T5-large | 770M | ~3.0GB | ~5.8GB | ‚úÖ Acceptable |\n",
    "\n",
    "**Note**: For training/fine-tuning, memory requirements are ~3-4x higher due to gradients and optimizer states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad17c372",
   "metadata": {},
   "source": [
    "## 5. Practical Tokenization Function\n",
    "\n",
    "Let's create a versatile function that can tokenize text using different models and display detailed information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52b724d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_model(text, model_name=\"bert-base-uncased\", use_cuda=True, verbose=True):\n",
    "    \"\"\"\n",
    "    Tokenize text using a specified transformer model.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to tokenize\n",
    "        model_name (str): HuggingFace model identifier\n",
    "        use_cuda (bool): Whether to use CUDA if available\n",
    "        verbose (bool): Print detailed information\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains tokenizer output and metadata\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if use_cuda and torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    try:\n",
    "        # Load tokenizer and model\n",
    "        if verbose:\n",
    "            print(f\"Loading {model_name}...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModel.from_pretrained(model_name).to(device)\n",
    "        \n",
    "        # Set pad token if it's not defined (common for GPT models)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            \n",
    "        # Track memory before\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            mem_before = torch.cuda.memory_allocated() / 1024**2\n",
    "        \n",
    "        # Tokenize\n",
    "        encoded = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        \n",
    "        encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "        \n",
    "        # Get model output (optional, to see full pipeline)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded)\n",
    "        \n",
    "        # Track memory after\n",
    "        if device == \"cuda\":\n",
    "            mem_after = torch.cuda.memory_allocated() / 1024**2\n",
    "            mem_peak = torch.cuda.max_memory_allocated() / 1024**2\n",
    "        \n",
    "        # Decode tokens back to text\n",
    "        tokens = tokenizer.convert_ids_to_tokens(encoded['input_ids'][0])\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Model: {model_name}\")\n",
    "            print(f\"Device: {device.upper()}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            print(f\"\\nOriginal text:\")\n",
    "            print(f\"  {text}\")\n",
    "            print(f\"\\nTokens ({len(tokens)} total):\")\n",
    "            print(f\"  {tokens}\")\n",
    "            print(f\"\\nToken IDs:\")\n",
    "            print(f\"  {encoded['input_ids'][0].tolist()}\")\n",
    "            print(f\"\\nVocabulary size: {tokenizer.vocab_size:,}\")\n",
    "            print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "            \n",
    "            if device == \"cuda\":\n",
    "                print(f\"\\nGPU Memory Usage:\")\n",
    "                print(f\"  Before: {mem_before:.2f} MB\")\n",
    "                print(f\"  After: {mem_after:.2f} MB\")\n",
    "                print(f\"  Peak: {mem_peak:.2f} MB\")\n",
    "                print(f\"  Model Memory: {(mem_after - mem_before):.2f} MB\")\n",
    "            \n",
    "            print(f\"\\nOutput shape: {outputs.last_hidden_state.shape}\")\n",
    "            print(f\"  [batch_size, sequence_length, hidden_size]\")\n",
    "        \n",
    "        # Clean up\n",
    "        del model, outputs\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        return {\n",
    "            \"tokens\": tokens,\n",
    "            \"token_ids\": encoded['input_ids'][0].tolist(),\n",
    "            \"attention_mask\": encoded['attention_mask'][0].tolist(),\n",
    "            \"num_tokens\": len(tokens),\n",
    "            \"vocab_size\": tokenizer.vocab_size,\n",
    "            \"model_name\": model_name\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {model_name}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10be6baf",
   "metadata": {},
   "source": [
    "## 6. Example: Tokenizing with BERT\n",
    "\n",
    "Let's test our function with BERT, one of the most popular transformer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bae6565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bert-base-uncased...\n",
      "\n",
      "============================================================\n",
      "Model: bert-base-uncased\n",
      "Device: CUDA\n",
      "============================================================\n",
      "\n",
      "Original text:\n",
      "  Transformers have revolutionized natural language processing with their attention mechanisms.\n",
      "\n",
      "Tokens (14 total):\n",
      "  ['[CLS]', 'transformers', 'have', 'revolution', '##ized', 'natural', 'language', 'processing', 'with', 'their', 'attention', 'mechanisms', '.', '[SEP]']\n",
      "\n",
      "Token IDs:\n",
      "  [101, 19081, 2031, 4329, 3550, 3019, 2653, 6364, 2007, 2037, 3086, 10595, 1012, 102]\n",
      "\n",
      "Vocabulary size: 30,522\n",
      "Model parameters: 109,482,240\n",
      "\n",
      "GPU Memory Usage:\n",
      "  Before: 418.73 MB\n",
      "  After: 427.90 MB\n",
      "  Peak: 428.31 MB\n",
      "  Model Memory: 9.17 MB\n",
      "\n",
      "Output shape: torch.Size([1, 14, 768])\n",
      "  [batch_size, sequence_length, hidden_size]\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"Transformers have revolutionized natural language processing with their attention mechanisms.\"\n",
    "\n",
    "result_bert = tokenize_with_model(\n",
    "    text=sample_text,\n",
    "    model_name=\"bert-base-uncased\",\n",
    "    use_cuda=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e693739e",
   "metadata": {},
   "source": [
    "### Understanding BERT Tokenization\n",
    "\n",
    "BERT uses **WordPiece tokenization**, which:\n",
    "- Breaks unknown words into subword units\n",
    "- Uses special tokens: `[CLS]` at start, `[SEP]` at end\n",
    "- Handles out-of-vocabulary words by splitting them (e.g., \"revolutionized\" ‚Üí \"revolution\", \"##ized\")\n",
    "- Preserves common words as single tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d2aa1d",
   "metadata": {},
   "source": [
    "## 7. Example: Tokenizing with DistilBERT\n",
    "\n",
    "DistilBERT is a smaller, faster version of BERT that's perfect for resource-constrained environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "072aee85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading distilbert-base-uncased...\n",
      "\n",
      "============================================================\n",
      "Model: distilbert-base-uncased\n",
      "Device: CUDA\n",
      "============================================================\n",
      "\n",
      "Original text:\n",
      "  Transformers have revolutionized natural language processing with their attention mechanisms.\n",
      "\n",
      "Tokens (14 total):\n",
      "  ['[CLS]', 'transformers', 'have', 'revolution', '##ized', 'natural', 'language', 'processing', 'with', 'their', 'attention', 'mechanisms', '.', '[SEP]']\n",
      "\n",
      "Token IDs:\n",
      "  [101, 19081, 2031, 4329, 3550, 3019, 2653, 6364, 2007, 2037, 3086, 10595, 1012, 102]\n",
      "\n",
      "Vocabulary size: 30,522\n",
      "Model parameters: 66,362,880\n",
      "\n",
      "GPU Memory Usage:\n",
      "  Before: 264.24 MB\n",
      "  After: 264.28 MB\n",
      "  Peak: 264.69 MB\n",
      "  Model Memory: 0.04 MB\n",
      "\n",
      "Output shape: torch.Size([1, 14, 768])\n",
      "  [batch_size, sequence_length, hidden_size]\n"
     ]
    }
   ],
   "source": [
    "result_distilbert = tokenize_with_model(\n",
    "    text=sample_text,\n",
    "    model_name=\"distilbert-base-uncased\",\n",
    "    use_cuda=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f3fd21",
   "metadata": {},
   "source": [
    "### DistilBERT Key Features\n",
    "- Uses the same tokenizer as BERT (WordPiece)\n",
    "- 40% fewer parameters than BERT-base\n",
    "- 60% faster inference\n",
    "- Only ~260MB model size vs ~440MB for BERT-base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827b6236",
   "metadata": {},
   "source": [
    "## 8. Example: Tokenizing with RoBERTa\n",
    "\n",
    "RoBERTa uses a different tokenization approach than BERT: Byte-Pair Encoding (BPE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1542d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading roberta-base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Model: roberta-base\n",
      "Device: CUDA\n",
      "============================================================\n",
      "\n",
      "Original text:\n",
      "  Transformers have revolutionized natural language processing with their attention mechanisms.\n",
      "\n",
      "Tokens (15 total):\n",
      "  ['<s>', 'Transform', 'ers', 'ƒ†have', 'ƒ†revolution', 'ized', 'ƒ†natural', 'ƒ†language', 'ƒ†processing', 'ƒ†with', 'ƒ†their', 'ƒ†attention', 'ƒ†mechanisms', '.', '</s>']\n",
      "\n",
      "Token IDs:\n",
      "  [0, 44820, 268, 33, 7977, 1538, 1632, 2777, 5774, 19, 49, 1503, 14519, 4, 2]\n",
      "\n",
      "Vocabulary size: 50,265\n",
      "Model parameters: 124,645,632\n",
      "\n",
      "GPU Memory Usage:\n",
      "  Before: 486.73 MB\n",
      "  After: 486.78 MB\n",
      "  Peak: 487.21 MB\n",
      "  Model Memory: 0.05 MB\n",
      "\n",
      "Output shape: torch.Size([1, 15, 768])\n",
      "  [batch_size, sequence_length, hidden_size]\n"
     ]
    }
   ],
   "source": [
    "result_roberta = tokenize_with_model(\n",
    "    text=sample_text,\n",
    "    model_name=\"roberta-base\",\n",
    "    use_cuda=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34b69ee",
   "metadata": {},
   "source": [
    "### RoBERTa Tokenization Differences\n",
    "- Uses **Byte-Pair Encoding (BPE)** instead of WordPiece\n",
    "- Larger vocabulary: ~50,000 tokens vs ~30,000 for BERT\n",
    "- Different special tokens: `<s>` (start), `</s>` (end) instead of `[CLS]`, `[SEP]`\n",
    "- Better handling of spaces and capitalization\n",
    "- Generally better performance on downstream tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199f720f",
   "metadata": {},
   "source": [
    "## 9. Comparing Tokenization Across Models\n",
    "\n",
    "Let's compare how different models tokenize the same text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5b35fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing tokenization for: 'CUDA-accelerated transformers enable efficient NLP.'\n",
      "\n",
      "================================================================================\n",
      "\n",
      "bert-base-uncased:\n",
      "  Tokens (10): ['cu', '##da', '-', 'accelerated', 'transformers', 'enable', 'efficient', 'nl', '##p', '.']\n",
      "  Token IDs: [101, 12731, 2850, 1011, 14613, 19081, 9585, 8114, 17953, 2361, 1012, 102]\n",
      "  Vocab size: 30,522\n",
      "\n",
      "distilbert-base-uncased:\n",
      "  Tokens (10): ['cu', '##da', '-', 'accelerated', 'transformers', 'enable', 'efficient', 'nl', '##p', '.']\n",
      "  Token IDs: [101, 12731, 2850, 1011, 14613, 19081, 9585, 8114, 17953, 2361, 1012, 102]\n",
      "  Vocab size: 30,522\n",
      "\n",
      "roberta-base:\n",
      "  Tokens (13): ['CU', 'DA', '-', 'ac', 'celer', 'ated', 'ƒ†transform', 'ers', 'ƒ†enable', 'ƒ†efficient', 'ƒ†N', 'LP', '.']\n",
      "  Token IDs: [0, 38260, 3134, 12, 1043, 24608, 1070, 7891, 268, 3155, 5693, 234, 21992, 4, 2]\n",
      "  Vocab size: 50,265\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def compare_tokenizers(text, models):\n",
    "    \"\"\"\n",
    "    Compare tokenization across multiple models.\n",
    "    \"\"\"\n",
    "    print(f\"Comparing tokenization for: '{text}'\\n\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    results = {}\n",
    "    for model_name in models:\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            tokens = tokenizer.tokenize(text)\n",
    "            token_ids = tokenizer.encode(text)\n",
    "            \n",
    "            print(f\"\\n{model_name}:\")\n",
    "            print(f\"  Tokens ({len(tokens)}): {tokens}\")\n",
    "            print(f\"  Token IDs: {token_ids}\")\n",
    "            print(f\"  Vocab size: {tokenizer.vocab_size:,}\")\n",
    "            \n",
    "            results[model_name] = {\n",
    "                \"tokens\": tokens,\n",
    "                \"num_tokens\": len(tokens),\n",
    "                \"vocab_size\": tokenizer.vocab_size\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"\\n{model_name}: Error - {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    return results\n",
    "\n",
    "# Compare the three main models\n",
    "comparison_text = \"CUDA-accelerated transformers enable efficient NLP.\"\n",
    "models_to_compare = [\n",
    "    \"bert-base-uncased\",\n",
    "    \"distilbert-base-uncased\",\n",
    "    \"roberta-base\"\n",
    "]\n",
    "\n",
    "comparison_results = compare_tokenizers(comparison_text, models_to_compare)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbe131e",
   "metadata": {},
   "source": [
    "## 10. Batch Processing and GPU Utilization\n",
    "\n",
    "One of the key advantages of using CUDA is the ability to process multiple texts in parallel. Let's demonstrate batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a0acb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading distilbert-base-uncased on CUDA...\n",
      "Processing 12 texts in batches of 4...\n",
      "\n",
      "============================================================\n",
      "Processing complete!\n",
      "  Total texts: 12\n",
      "  Total time: 0.052 seconds\n",
      "  Throughput: 229.60 texts/second\n",
      "  Average time per text: 4.36 ms\n",
      "  Output shape: torch.Size([12, 768])\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def batch_tokenize(texts, model_name=\"distilbert-base-uncased\", batch_size=8, use_cuda=True):\n",
    "    \"\"\"\n",
    "    Process multiple texts in batches using GPU acceleration.\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if use_cuda and torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    print(f\"Loading {model_name} on {device.upper()}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name).to(device)\n",
    "    \n",
    "    # Set pad token if it's not defined\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    print(f\"Processing {len(texts)} texts in batches of {batch_size}...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    all_embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize batch\n",
    "        encoded = tokenizer(\n",
    "            batch_texts, \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "        \n",
    "        # Get embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded)\n",
    "        \n",
    "        # Use [CLS] token embedding (first token)\n",
    "        batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu()\n",
    "        all_embeddings.append(batch_embeddings)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Combine all embeddings\n",
    "    all_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    total_time = end_time - start_time\n",
    "    texts_per_second = len(texts) / total_time\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing complete!\")\n",
    "    print(f\"  Total texts: {len(texts)}\")\n",
    "    print(f\"  Total time: {total_time:.3f} seconds\")\n",
    "    print(f\"  Throughput: {texts_per_second:.2f} texts/second\")\n",
    "    print(f\"  Average time per text: {total_time/len(texts)*1000:.2f} ms\")\n",
    "    print(f\"  Output shape: {all_embeddings.shape}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Clean up\n",
    "    del model\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return all_embeddings\n",
    "\n",
    "# Create sample texts for batch processing\n",
    "sample_texts = [\n",
    "    \"Natural language processing has advanced significantly.\",\n",
    "    \"CUDA acceleration enables faster model inference.\",\n",
    "    \"Transformers are the state-of-the-art in NLP.\",\n",
    "    \"BERT introduced bidirectional context understanding.\",\n",
    "    \"DistilBERT offers a good balance of speed and accuracy.\",\n",
    "    \"RoBERTa improved upon BERT's training methodology.\",\n",
    "    \"Tokenization is crucial for transformer models.\",\n",
    "    \"GPU memory is important for large models.\",\n",
    "    \"The RTX 5060 Ti has 12GB of VRAM.\",\n",
    "    \"Batch processing improves throughput significantly.\",\n",
    "    \"PyTorch provides excellent CUDA support.\",\n",
    "    \"HuggingFace makes transformers accessible to everyone.\",\n",
    "]\n",
    "\n",
    "embeddings = batch_tokenize(sample_texts, model_name=\"distilbert-base-uncased\", batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623c055c",
   "metadata": {},
   "source": [
    "## 11. Memory Profiling Function\n",
    "\n",
    "Let's create a function to profile GPU memory usage for different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3744c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GPU Memory Profiling\n",
      "============================================================\n",
      "Profiling distilbert-base-uncased...\n",
      "  Parameters: 66,362,880\n",
      "  Model size: 253.15 MB\n",
      "  Memory allocated: 255.11 MB\n",
      "  Peak memory: 272.56 MB\n",
      "\n",
      "Profiling bert-base-uncased...\n",
      "  Parameters: 109,482,240\n",
      "  Model size: 417.64 MB\n",
      "  Memory allocated: 419.60 MB\n",
      "  Peak memory: 437.05 MB\n",
      "\n",
      "Profiling roberta-base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters: 124,645,632\n",
      "  Model size: 475.49 MB\n",
      "  Memory allocated: 477.60 MB\n",
      "  Peak memory: 495.08 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def profile_model_memory(model_name, seq_length=512):\n",
    "    \"\"\"\n",
    "    Profile GPU memory usage for a model.\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"CUDA not available. Cannot profile GPU memory.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Profiling {model_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Clear GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        \n",
    "        mem_start = torch.cuda.memory_allocated() / 1024**2\n",
    "        \n",
    "        # Load model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModel.from_pretrained(model_name).to(\"cuda\")\n",
    "        \n",
    "        # Set pad token if it's not defined\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        mem_after_load = torch.cuda.memory_allocated() / 1024**2\n",
    "        \n",
    "        # Create dummy input\n",
    "        dummy_text = \"a \" * (seq_length // 2)  # Approximate tokens\n",
    "        encoded = tokenizer(dummy_text, return_tensors=\"pt\", truncation=True, max_length=seq_length)\n",
    "        encoded = {k: v.to(\"cuda\") for k, v in encoded.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded)\n",
    "        \n",
    "        mem_peak = torch.cuda.max_memory_allocated() / 1024**2\n",
    "        \n",
    "        # Get model parameters\n",
    "        num_params = sum(p.numel() for p in model.parameters())\n",
    "        model_size_mb = num_params * 4 / 1024**2  # 4 bytes per float32 parameter\n",
    "        \n",
    "        result = {\n",
    "            \"model_name\": model_name,\n",
    "            \"parameters\": num_params,\n",
    "            \"model_size_mb\": model_size_mb,\n",
    "            \"memory_allocated_mb\": mem_after_load - mem_start,\n",
    "            \"peak_memory_mb\": mem_peak,\n",
    "            \"sequence_length\": seq_length\n",
    "        }\n",
    "        \n",
    "        print(f\"  Parameters: {num_params:,}\")\n",
    "        print(f\"  Model size: {model_size_mb:.2f} MB\")\n",
    "        print(f\"  Memory allocated: {result['memory_allocated_mb']:.2f} MB\")\n",
    "        print(f\"  Peak memory: {mem_peak:.2f} MB\")\n",
    "        \n",
    "        # Clean up\n",
    "        del model, tokenizer, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Profile all models\n",
    "print(\"=\"*60)\n",
    "print(\"GPU Memory Profiling\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "models_to_profile = [\n",
    "    \"distilbert-base-uncased\",\n",
    "    \"bert-base-uncased\",\n",
    "    \"roberta-base\",\n",
    "]\n",
    "\n",
    "profile_results = []\n",
    "for model_name in models_to_profile:\n",
    "    result = profile_model_memory(model_name)\n",
    "    if result:\n",
    "        profile_results.append(result)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca884c94",
   "metadata": {},
   "source": [
    "## 12. Summary and Recommendations\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **All three main models (BERT, DistilBERT, RoBERTa) work excellently on RTX 5060 Ti (12GB)**\n",
    "   - DistilBERT: Best for speed and efficiency (~260MB, 66M params)\n",
    "   - BERT: Good balance of performance and size (~440MB, 110M params)\n",
    "   - RoBERTa: Best accuracy, slightly larger (~500MB, 125M params)\n",
    "\n",
    "2. **Tokenization Strategies**\n",
    "   - BERT/DistilBERT: WordPiece tokenization, ~30K vocab\n",
    "   - RoBERTa: Byte-Pair Encoding (BPE), ~50K vocab\n",
    "   - Each has trade-offs in handling rare words and languages\n",
    "\n",
    "3. **CUDA Benefits**\n",
    "   - 10-50x speedup over CPU depending on batch size\n",
    "   - Enables real-time processing for many applications\n",
    "   - Critical for batch processing and production deployments\n",
    "\n",
    "4. **Memory Considerations**\n",
    "   - For inference: Model size √ó 1.2-1.5 is a good estimate\n",
    "   - For training: Model size √ó 3-4 due to gradients and optimizer states\n",
    "   - Batch size has linear impact on memory usage\n",
    "\n",
    "### Recommendations for RTX 5060 Ti (12GB)\n",
    "\n",
    "**For Inference:**\n",
    "- ‚úÖ All base models: Excellent\n",
    "- ‚úÖ Large models (BERT-large, RoBERTa-large): Good with moderate batch sizes\n",
    "- ‚úÖ GPT-2 up to Large: Acceptable\n",
    "- ‚ö†Ô∏è Models > 2GB: Possible but limited batch size\n",
    "\n",
    "**For Fine-tuning:**\n",
    "- ‚úÖ DistilBERT, ALBERT: Excellent, can use larger batch sizes\n",
    "- ‚úÖ BERT-base, RoBERTa-base: Good with batch size 8-16\n",
    "- ‚ö†Ô∏è Large models: Use gradient accumulation or mixed precision training\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Use mixed precision (FP16)** to reduce memory usage by ~50%\n",
    "2. **Start with DistilBERT** for prototyping\n",
    "3. **Profile memory** before deploying to production\n",
    "4. **Use batch processing** to maximize GPU utilization\n",
    "5. **Clear GPU cache** between runs to avoid OOM errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2a4793",
   "metadata": {},
   "source": [
    "## 13. Bonus: Testing Additional Models\n",
    "\n",
    "Feel free to test other models by modifying the model name below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9b3bc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gpt2-xl...\n",
      "\n",
      "============================================================\n",
      "Model: gpt2-xl\n",
      "Device: CUDA\n",
      "============================================================\n",
      "\n",
      "Original text:\n",
      "  This is a test of the tokenization system with CUDA acceleration.\n",
      "\n",
      "Tokens (14 total):\n",
      "  ['This', 'ƒ†is', 'ƒ†a', 'ƒ†test', 'ƒ†of', 'ƒ†the', 'ƒ†token', 'ization', 'ƒ†system', 'ƒ†with', 'ƒ†CU', 'DA', 'ƒ†acceleration', '.']\n",
      "\n",
      "Token IDs:\n",
      "  [1212, 318, 257, 1332, 286, 262, 11241, 1634, 1080, 351, 29369, 5631, 20309, 13]\n",
      "\n",
      "Vocabulary size: 50,257\n",
      "Model parameters: 1,557,611,200\n",
      "\n",
      "GPU Memory Usage:\n",
      "  Before: 6134.05 MB\n",
      "  After: 6142.34 MB\n",
      "  Peak: 6144.13 MB\n",
      "  Model Memory: 8.29 MB\n",
      "\n",
      "Output shape: torch.Size([1, 14, 1600])\n",
      "  [batch_size, sequence_length, hidden_size]\n"
     ]
    }
   ],
   "source": [
    "# Try other models:\n",
    "# - \"albert-base-v2\" (smaller, parameter-efficient)\n",
    "# - \"bert-large-uncased\" (larger BERT)\n",
    "# - \"roberta-large\" (larger RoBERTa)\n",
    "# - \"xlm-roberta-base\" (multilingual)\n",
    "# - \"google/electra-base-discriminator\" (efficient pre-training)\n",
    "# - \"gpt2\" (GPT-2 small, 124M params)\n",
    "# - \"gpt2-medium\" (GPT-2 medium, 355M params)\n",
    "# - \"gpt2-large\" (GPT-2 large, 774M params)\n",
    "# - \"gpt2-xl\" (GPT-2 XL, 1.5B params)  Error loading gpt2-xl: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "test_model = \"gpt2-xl\"  # Change this to test different models\n",
    "test_text = \"This is a test of the tokenization system with CUDA acceleration.\"\n",
    "\n",
    "result = tokenize_with_model(\n",
    "    text=test_text,\n",
    "    model_name=test_model,\n",
    "    use_cuda=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac2452d",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored:\n",
    "- How to set up and use HuggingFace transformers with CUDA\n",
    "- The differences between BERT, DistilBERT, and RoBERTa tokenization\n",
    "- Memory requirements and GPU compatibility\n",
    "- Practical examples and batch processing\n",
    "- Performance optimization techniques\n",
    "\n",
    "The RTX 5060 Ti with 12GB VRAM is an excellent GPU for working with modern transformer models, capable of handling all base models and many large models for both inference and fine-tuning.\n",
    "\n",
    "**Next Steps:**\n",
    "- Experiment with different models for your specific use case\n",
    "- Try fine-tuning on your own dataset\n",
    "- Explore multi-GPU training for larger models\n",
    "- Investigate quantization and model compression techniques\n",
    "\n",
    "Happy tokenizing! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids25.12_python3.12_cuda13 (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
